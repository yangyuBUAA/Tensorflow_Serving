if [ ! -d "/opt/app/tfserving" ]; then
    mv ../tfserving /opt/app/
fi
nohup docker run --gpus='"device=0"'  -p 8501:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving1_1 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf1_1.log &
nohup docker run --gpus='"device=0"'  -p 8502:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving1_2 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf1_2.log &
nohup docker run --gpus='"device=0"'  -p 8503:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving1_3 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf1_3.log &
nohup docker run --gpus='"device=0"'  -p 8504:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving1_4 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf1_4.log &
nohup docker run --gpus='"device=0"'  -p 8505:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving1_5 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf1_5.log &
nohup docker run --gpus='"device=0"'  -p 8506:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving1_6 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf1_6.log &

nohup docker run --gpus='"device=1"'  -p 8507:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving2_1 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf2_1.log & 
nohup docker run --gpus='"device=1"'  -p 8508:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving2_2 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf2_2.log & 
nohup docker run --gpus='"device=1"'  -p 8509:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving2_3 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf2_3.log & 
nohup docker run --gpus='"device=1"'  -p 8510:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving2_4 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf2_4.log & 
nohup docker run --gpus='"device=1"'  -p 8511:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving2_5 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf2_5.log & 
nohup docker run --gpus='"device=1"'  -p 8512:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving2_6 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf2_6.log & 

nohup docker run --gpus='"device=2"'  -p 8513:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving3_1 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf3_1.log &
nohup docker run --gpus='"device=2"'  -p 8514:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving3_2 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf3_2.log &
nohup docker run --gpus='"device=2"'  -p 8515:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving3_3 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf3_3.log &
nohup docker run --gpus='"device=2"'  -p 8516:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving3_4 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf3_4.log &
nohup docker run --gpus='"device=2"'  -p 8517:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving3_5 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf3_5.log &
nohup docker run --gpus='"device=2"'  -p 8518:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving3_6 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf3_6.log &

nohup docker run --gpus='"device=3"'  -p 8519:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving4_1 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf4_1.log &
nohup docker run --gpus='"device=3"'  -p 8520:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving4_2 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf4_2.log &
nohup docker run --gpus='"device=3"'  -p 8521:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving4_3 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf4_3.log &
nohup docker run --gpus='"device=3"'  -p 8522:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving4_4 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf4_4.log &
nohup docker run --gpus='"device=3"'  -p 8523:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving4_5 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf4_5.log &
nohup docker run --gpus='"device=3"'  -p 8524:8501 -v /opt/app/tfserving/:/models/MultiModel --name=tfserving4_6 -t tensorflow/serving:latest-gpu --config=cuda --model_config_file=/models/MultiModel/model.config --enable_batching=true --batching_parameters_file=/models/MultiModel/batch.config --platform_config_file=/models/MultiModel/platform.config > tf4_6.log &
